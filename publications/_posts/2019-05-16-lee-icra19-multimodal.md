---
layout: publication
title: "Making Sense of Vision and Touch: Self-Supervised Learning of Multimodal Representations for 
Contact-Rich Tasks"
authors: "Michelle A. Lee*, Yuke Zhu*, Krishnan Srinivasan, Parth Shah, Silvio Savarese, Li Fei-Fei, Animesh Garg, Jeannette Bohg"
pub_info_name: "IEEE International Conference on Robotics and Automation (ICRA) - Best Paper Award"
pub_info_date: May 2019
excerpt: text text text
images:
  thumb: lee-arxiv18-multimodal.png
  main: lee-arxiv18-multimodal.png
paper_link: "https://arxiv.org/abs/1810.10191"
webpage_link: "https://sites.google.com/view/visionandtouch"
video_link: "https://youtu.be/TjwDJ_R2204"
---
Contact-rich manipulation tasks in unstructured environments often require both haptic and visual feedback. However, it is non-trivial to manually design a robot controller that combines modalities with very different characteristics. While deep reinforcement learning has shown success in learning control policies for high-dimensional inputs, these algorithms are generally intractable to deploy on real robots due to sample complexity. We use self-supervision to learn a compact and multimodal representation of our sensory inputs, which can then be used to improve the sample efficiency of our policy learning. We evaluate our method on a peg insertion task, generalizing over different geometry, configurations, and clearances, while being robust to external perturbations. Results for simulated and real robot experiments are presented.
