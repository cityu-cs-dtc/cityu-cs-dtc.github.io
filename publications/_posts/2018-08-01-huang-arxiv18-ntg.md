---
layout: publication
title: "Neural Task Graphs: Generalizing to Unseen Tasks from a Single Video Demonstration"
authors: "De-an Huang*, Suraj Nair*, Danfei Xu*, Yuke Zhu, Animesh Garg, Li Fei-Fei, Silvio Savarese, Juan Carlos Niebles"
pub_info_name: "arXiv preprint"
pub_info_date: August 2018
excerpt: text text text
images:
  thumb: huang-arxiv18-ntg.jpg
  main: huang-arxiv18-ntg.jpg
paper_link: "https://arxiv.org/abs/1807.03480"
webpage_link: "https://sites.google.com/view/task-oriented-grasp"
---
Our goal is for a robot to execute a previously unseen task based on a single video demonstration of the task. The success of our approach relies on the principle of transferring knowledge from seen tasks to unseen ones with similar semantics. More importantly, we hypothesize that to successfully execute a complex task from a single video demonstration, it is necessary to explicitly incorporate compositionality to the model. To test our hypothesis, we propose Neural Task Graph (NTG) Networks, which use task graph as the intermediate representation to modularize the representations of both the video demonstration and the derived policy. We show this formulation achieves strong inter-task generalization on two complex tasks: Block Stacking in BulletPhysics and Object Collection in AI2-THOR. We further show that the same principle is applicable to real-world videos. We show that NTG can improve data efficiency of few-shot activity understanding in the Breakfast Dataset. 
